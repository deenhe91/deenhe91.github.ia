<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>classification</title>
  <meta name="description" content="30 category classification - better with defined features or brute force?">

  <!-- CSS files -->
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/main.css">

  <link rel="canonical" href="/articles/2016-05/classifiers">
  <link rel="alternate" type="application/rss+xml" title="Hannah" href=" /feed.xml " />

  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="/favicon.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="/favicon.png">
</head>


<body>
  <div class="row">
    <div class="col s12 m3">
      <div class="table cover">
        

<div class="cover-card table-cell table-middle">
  
  <img src="/img/madfun2.jpg" alt="" class="avatar">
  
  <a href="/" class="author_name">Hannah Deen</a>
  <span class="author_job">data scientist: Amsterdam, London, NYC</span>
  <span class="author_bio mbm"></span>
  <nav class="nav">
    <ul class="nav-list">
       
      <li class="nav-item">
        <a href="/archive/">Archive</a>
        <span>/</span>
      </li>
          
      <li class="nav-item">
        <a href="/categories/">Categories</a>
        <span>/</span>
      </li>
            
      <li class="nav-item">
        <a href="/tags/">Tags</a>
      </li>
       
    </ul>
  </nav>
  <div class="social-links">
  <ul>
    <li><a href="mailto:hannah.deen91@gmail.com" class="social-link-item" target="_blank"><i class="fa fa-fw fa-envelope"></i></a></li>
    
    
    
    <li><a href="http://linkedin.com/in/deenhe" class="social-link-item" target="_blank"><i class="fa fa-fw fa-linkedin"></i></a></li>
    
    
    
    <li><a href="http://github.com/deenhe91" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</div>

</div>

      </div>
    </div>
    <div class="col s12 m9">
      <div class="post-listing">
        <div class="back-home">
  <a href="/">< back Home.</a>
</div>

<div class="post-image-feature">
  <img class="feature-image" src=
  
  "http://www.biologydiscussion.com/wp-content/uploads/2015/10/clip_image004_thumb223.jpg"
  
  alt="classification feature image">

  
</div><!-- /.image-wrap -->



<div id="post">
  <header class="post-header">
    <h1 title="classification">classification</h1>
    <span class="post-meta">
      <span class="post-date">
        13 MAY 2016
      </span>
      •
      <span class="read-time" title="Estimated read time">
  
  
    8 mins read
  
</span>

    </span>

  </header>

  <article class="post-content">
    <h3 id="before-i-start">before I start…</h3>
<p>I want to highlight the two biggest personal lessons I learnt at Metis were learnt during this project.</p>

<h3 id="procrastination-will-kill-you">procrastination will kill you.</h3>

<p>I did not know what to do. And I did not stick to any idea that I had. Initially we started in groups with vague aims of classifying a genre into a subgenre. We went to the spotify music hackathon, bright-eyed and bushy-tailed and ready for adventure. Though the hackathon itself was fab and the speakers were inspiring - we didn’t figure out our subgenre problem. The million song dataset wouldn’t fit on my AWS, <em>and</em> the spotify API didn’t return the things that we were interested in. The fields of interest were empty. Additionally, the awesome python libraries that we could use to analyse audio files was not cooperating with my <code>python3</code>. All of these barriers would have been circumventable if only I had believed in the cause. I said yes to this because I was excited to work with music but classifying genres into subgenres was not something I felt I could put my heart into. And so all of these hiccups felt all the more monumental.</p>

<p>With retrospect I could have done some pretty neat classification problems surrounding music.
and I will probably explore these now I have some ‘free’ time (ahum. job-hunting.) but at the time these did not come to me.</p>

<p>I looked up datasets and debated playing with calcium signalling data (<a href="http://neurofinder.codeneuro.org/">is a neuron of interest or not?</a>), or are mushrooms edible or not, or even does this person have Parkinson’s or not? Some SUPER interesting things kicking about there but procrastination killed them all. And 36 hours before presentation time, I decided to classify leaf images to corresponding tree species. This was fine, but given the time strain I had very little opportunity to dig deeper and do some interesting analysis or to seriously consider the problem or uses of this classifier.</p>

<p>Moral of the story: Find a project and <em>stick to it</em>. You can make something interesting out of everything.</p>

<h3 id="dont-try-and-change-the-world-with-every-project">don’t try and change the world with every project.</h3>

<p>You will <strong>always</strong> be disappointed. <strong>Always</strong>. One of my main issues is having to make everything I do ‘important’ to justify putting time into it. But ultimately this course is about learning the ropes. Taking on seemingly less serious topics will allow faster blooming of my technical capabilities.
I have always appreciated the little things and I get excited by small logic problems and even things like data cleaning but if I have to present a project I have had a tendency to overthink and search for the much much bigger picture. This has made it difficult to get excited about things that just have a simple bigger picture. Or projects that are only focused on ‘increasing revenue’. But I have realised how detrimental this is. For the leaf project I went to lengths to make up a story about the APOCALYPSE! Are you ready?</p>

<p><em>…it is 50 years in the future, and infrastructure has collapsed. It is more and more difficult to find reliable, safe and affordable medicine and so the people are turning back to nature for their remedies. For this reason they need a classifier to tell them, based on an image of the leaf, what the tree species is and whether that plant is medicinally useful to you…</em></p>

<p>Yes. Our fantastic teacher Vinny pointed out that actually we don’t need an apocalyptic future to justify a leaf classifier and it can be useful for education, park rangers and many other entirely relevant things.</p>

<h2 id="the-project">the project</h2>

<p>I got this dataset from <a href="http://archive.ics.uci.edu/ml/datasets/Leaf">UCI</a> along with a paper by Pedro explaining his logic and process. I used openCV to extract some of the features he mentioned in his thesis (all of which were present in the dataset itself) to get a feel for the openCV package and learn a bit about image processing.</p>

<h4 id="cleaning-up-the-data">cleaning up the data</h4>

<p>This data was pretty clean already so it was just a case of organising and understanding it. I replaced string nans with <code>float(nan)</code> so that <code>pandas</code> would recognise them as non-values.</p>

<p>I grouped my data by species to have a look at the summary of differed variables per species. To see whether something was largely <code>nans</code> and just to understand the metrics a little better because they consisted of things like ‘Eccentricity’, ‘Solidity’ and ‘Isoperimetric Factor’… (see end for explanation of these terms).</p>

<p>I created a reduced dataset that didn’t have things in it I wasn’t planning on using in my analysis and I <code>pickled</code> that so that I could just load it up again if I quit the programme.</p>

<p><strong>The test-train split</strong>. I had 30 species and only 8-15 specimens per species so it was important to do a stratified split so that all the species were present in both the training and the test set.</p>

<p>I used <code>seaborn</code> and <code>matplotlib</code> to plot some of the features(variables) in the training set.</p>

<p>I used feature importance analysis to pull out the most important features as lots of variables means lots of dimensions which makes it harder to glean relevant insights.</p>

<p>It couldn’t hurt to prune some out.</p>

<p><img src="https://github.com/deenhe91/deenhe91.github.io/blob/master/images/class_features.png?raw=true" alt="" /></p>

<p>I tried running my classifier models with the top 9 (and then 10, and then 11) features but my accuracy score for each model dropped a lot, some by 20 points. So I stuck with all 16.</p>

<p>A little sidenote - I plotted a scatter matrix for my features which is just a matrix of scatter plots that shows you how each pair of variables are related. Down the diagonal (where the one variable would be compared with itself) is a KDE plot for that particular variable. That just shows the distribution within the variable. This is a great and reasonable quick way to visually understand your data and the relaitonships within it - that is, if you don’t have more than 10 or 15 variables..</p>

<p><img src="https://github.com/deenhe91/deenhe91.github.io/blob/master/images/ClassScatter_matrix.png?raw=true" alt="" /></p>

<h3 id="a-little-summary-of-the-classifiers">A little summary of the classifiers</h3>

<h5 id="logistic-regression">LOGISTIC REGRESSION</h5>

<p>Logistic regression is regression where the dependent variable (the thing you are predicting) is categorical (splits into categories). This is most effectively used, or designed for, binary models. Usually in cases of multiple unordered categories, the regression model to use is <em>Multinomial Linear Regression</em> which doesn’t have a handy function in scikit learn. When applied to a problem with multiple categories, like the leaf dataset, a logistic regression analysis will assess the probability that something is in one category compared to all other categories.</p>

<p>Logistic regression assumes a clear cut off. The green line in the example below is the probability of a data point being ‘a case’ given 1 feature. The blue dots are cases and the red ones are not. A classic example the probability of a student passing a test given the number of hours they spend studying for that test. Blue is pass, red is fail, and the numbers on the x axis are the number of hours spent studying. It seems logical that the probability of passing increases with study hours.</p>

<p><img src="https://github.com/deenhe91/deenhe91.github.io/blob/master/images/140.png?raw=true" alt="" /></p>

<h5 id="svm">SVM</h5>

<p>SVM stands for <strong>Support Vector Machine</strong>. It’s basic idea is that you define a decision boundary using the ‘widest street’ approach. This means that you want to draw a line that has on either side of it, the biggest margin possible between the two categories. Wihout a <strong>kernel</strong>, SVMs are only really useful for data that is linearly divided. It looks a little like this:</p>

<p><img src="http://3.bp.blogspot.com/_UqlrkHvPijw/TJupAi2ztMI/AAAAAAAAAFI/6EVz1pmA1vs/s1600/svm.gif" alt="" /></p>

<p>Using a kernel means drawing the line in a higher dimensional space and then reducing the dimensionality so that the line no longer looks straight. 
<img src="http://www.sbaban.org/wp-content/uploads/2013/11/svm.jpg" alt="" />
Simple, right? When you use kernels correctly SVMs can be super useful for all kinds of classification problems. But as always you need to be careful of overfitting.</p>

<p>I find that this <a href="https://www.youtube.com/watch?v=_PwhiWxHK8o">MIT lecture</a> is really helpful. Chalkboards are the best.</p>

<h5 id="decision-trees-and-random-forests">DECISION TREES and RANDOM FORESTS</h5>

<p>Decision trees and random forests are probably my favourite type of classification algorithm because they are really intuitive  but also really effective in lots of situations. You run the risk of overfitting, but random forests help there.</p>

<p>Decision trees use if-then statements to define patterns in the data. A decision tree consists of a starting point, where you have ALL your training data, and then you split that data by asking a simple yes/no question and splitting the data accordingly. For example, is your leaf longer than 10cm? Yes: contains all leaves longer than 10cm, and No: contains all leaves shorter than 10cm. You set how many questions you want to ask, or in the correct lingo, how many levels you want the tree to contain and <em>Voilá</em>! You end up with categories into which new leaves will be classified. With labeled data, the accuracy score is the probability that a leaf is correctly classified.</p>

<p>Random Forests just do the same process multiple times (lots of decision trees) and then take the mode or mean prediction of the sum of the trees. The good thing about Random Forests is that they can reduce the risk of overfitting that can happen with Decision Trees.</p>

<p>LOW AND BEHOLD, for my leaf dataset the Random Forest classifier was the most accurate. Which was just perfect. Because leaves. And trees. And forests. Geddit?</p>

<p>Here’s the breakdown, where the y axis shows the accuracy score:</p>

<p><img src="https://github.com/deenhe91/deenhe91.github.io/blob/master/images/class_accuracy.png?raw=true" alt="" /></p>


  </article>
</div>

<div class="share-buttons">
  <h6>Share on: </h6>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=/articles/2016-05/classifiers" class="twitter btn" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=/articles/2016-05/classifiers" class="facebook btn" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=/articles/2016-05/classifiers" class="google-plus btn" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
    <li>
      <a href="https://news.ycombinator.com/submitlink?u=/articles/2016-05/classifiers" class="hacker-news btn" title="Share on Hacker News"><i class="fa fa-hacker-news"></i><span> Hacker News</span></a>
    </li>
    <li>
      <a href="https://www.reddit.com/submit?url=/articles/2016-05/classifiers" class="reddit btn" title="Share on Reddit"><i class="fa fa-reddit"></i><span> Reddit</span></a>
    </li>
  </ul>
</div><!-- end share-buttons -->



        <footer>
  &copy; 2017 Hannah Deen. Powered by <a href="http://jekyllrb.com/">Jekyll</a>, <a href="http://github.com/renyuanz/leonids/">leonids theme</a> made with <i class="fa fa-heart heart-icon"></i>
</footer>

      </div>
    </div>
  </div>
  <script type="text/javascript" src="/js/jquery-2.1.4.min.js"></script>
<script type="text/javascript" src="/js/main.js"></script>


</body>
</html>
