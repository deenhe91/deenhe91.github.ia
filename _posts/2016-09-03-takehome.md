---

layout: post
title: TAKE HOME CHALLENGE
excerpt: "using Google, APIs and loads of image classification"
categories: [Data Thoughts]
comments: true
image:
  feature: https://github.com/deenhe91/deenhe91.github.io/blob/master/images/code.jpg?raw=true
  credit: 
  creditlink:

---

# my take-home challenge

Last week I interviewed with a tech company near Utrecht and they gave me a 7-day take home challenge to prove my worth before we could talk about hiring. The challenge involved using google's machine learning APIs, specifically the [Google Vision API](https://cloud.google.com/vision/) to classify an arbitrary number of images (1,000-10,000) from a site with publicly available images. My experience with app building and servers is more limited than with building models but my interest in it is just as strong, so I was excited at the challenge.

I needed to understand all the parts, moving and static of this machine, so I drew out some workflows. I knew I needed to get images, which meant using an image website API. Those images would have to be sent to Google and classified somehow. The results of that needed to be stored in a database (the company asked me to use [Firebase](https://firebase.google.com/), another new thing) and then that database would have to be accessed by a user looking up a specific animal in a simple front-end situation.

Here are some of my drawings: 

## flickr and the images

I chose flickr, specifically the get group photos [API](flickr.com/services/api/explore/flickr.groups.pools.getPhotos)(even though I had to create a yahoo account in order to use it...) The quality of the images available is much higher than say, _imgur_, and the developer toolkit, or _'App Garden'_ is pretty well documented and easy to implement.


```python

import requests

flickr_key = '<YOUR ACCOUNT KEY>'
flickr_secret = '<YOUR ACCOUNT SECRET>'

flickr_file = []

for i in range(10):
	r = requests.get('https://api.flickr.com/services/rest/?method=flickr.groups.pools.getPhotos&api_key=<API KEY>&group_id=<GROUP ID>&per_page=500&page='+str(i)+'&format=json&nojsoncallback=1')

	flickr_json = r.json()
	flickr_file.append(flickr_json)
```

Using this API returned lots of information regarding each image within a specified 'group', which is basically a collection of images that you can add to with a common theme. I then parsed the information that was relevant to me and saved it, so I wouldn't have to overstay my flickr-api-welcome if it went wrong.


```python

img_info = flickr_list['photos']['photo']

with open('flickr.json', 'w') as outfile:
	json.dump(flickr_file, outfile)

```

I then had to compile URLs that pointed to each image so that they could be downloaded, but the format that flickr provided didn't work. However, looking at the page source...

![](https://github.com/deenhe91/deenhe91.github.io/blob/master/images/page_source.png?raw=true)

we can see the true format for the image URL. The format looks like this:

`https://farm{farm-id}.staticflickr.com/{server-id}/{id}_{secret}.jpg`

and all of the required fields were in the flickr api response. Hoorah!

The google vision API requires you to _either_ use images that are stored in Google Storage, _or_ send in the base64 encoded image as a string. So I downloaded the image at each URL using `shutil`, base64 encoded it using `b64` and then sent that string to Google.

```python

import shutil
import b64

b64_strings = [] # create list of b64_strings

for url in urls:

	response = requests.get(url, stream=True)
	
	with open('img.png', 'wb') as f:
		shutil.copyfileobj(response.raw, f)

	with open('img.png', 'rb') as f:
		encoded_string = base64.b64encode(f.read())

		b64_strings.append(encoded_string)

del response
```

Actually, I started off going through the process step by step, and saving along the way with the intention of turning it into a smooth running pipeline at the end. First I wanted to test whether each step worked without having to repeat any working processes. 

Because the name of the stored image, `img.png` remained the same, it was replaced at each iteration and I didn't hold onto anything in memory that I didn't need to.

## google and the vision

So I had my base64 encoded images ready for processing. In order to use Google Vision I needed to have a project and a key

In order to use the API from my remote server, I had to create an environmental variable called 'GOOGLE_APPLICATION_CREDENTIALS' pointing to the file where my credentials were stored. You can see below, you just use `os` to do this. When you create your credentials with Google, a json file is automatically downloaded to your computer so just make sure that's in the right directory. 

```python

import os 
from googleapiclient import discovery 
from oauth2client.client import GoogleCredentials 

DISCOVERY_URL = 'https://vision.googleapis.com/$discovery/rest?version=v1'
# because I was using the google vision API

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '<json file where your credentials are stored>'

credentials = GoogleCredentials.get_application_default()	

service = discovery.build('vision', 'v1', credentials=credentials, discoveryServiceUrl=DISCOVERY_URL)

```

 
You can play around with the parameters a bit and specify what kind of detection you want (e.g. face, landmark, text). But I was just interested in animals so I set it to `'LABEL_DETECTION'`. I then set the parameters to max 5 labels per image, and saved results in a python dictionary along with the score of that label and the url, so that when it came to front end, I could return results by score and show the corresponding image.

Every now and again a differently formatted or failed result would be returned so i introduced a conditional if statement (if ‘labelAnnotations’ in blah blah: ) into the google api stage where it would also print(‘skipped string ’ + str(i)) so i could see how many were being skipped. that way, if it was loads, i would know that something is wrong with my code or the request.

```python

googlers = []

for string in b64_strings:

	service_request = service.images().annotate(body={'requests': [{'image': {'content': string.decode('UTF-8')},'features': [{'type': 'LABEL_DETECTION','maxResults': 5}]}]})

	response = service_request.execute()

	if 'labelAnnotations' in response['responses'][0]:
		labels = response['responses'][0]['labelAnnotations']
		print('string :'+ str(i))
	else:
		print('SKIPPED string : ' +str(i))
	googlers.append(labels)

```

#### Overall, the google Vision API appears to do a good job. A couple of exceptions?

![](https://github.com/deenhe91/deenhe91.github.io/blob/master/images/maitai.jpg?raw=true)

\n
\n

#### Google thinks this is not a butterfly, not butterflies or a butterfly feeder, but a Mai Tai..

![](https://github.com/deenhe91/deenhe91.github.io/blob/master/images/real_maitai.jpg?raw=true)

firebase api

front end