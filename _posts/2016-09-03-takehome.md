---

layout: post
title: a couple of apis, an app and some animals
excerpt: "using Google, APIs and loads of image classification"
categories: [Data Thoughts]
comments: true
image:
  feature: https://github.com/deenhe91/deenhe91.github.io/blob/master/images/code.jpg?raw=true
  credit: 
  creditlink:

---

# my take-home challenge

Last week I interviewed with a tech company near Utrecht and they gave me a 7-day take home challenge to prove my worth before we could talk about hiring. The challenge involved using google's machine learning APIs, specifically the [Google Vision API](https://cloud.google.com/vision/) to classify an arbitrary number of images (1,000-10,000) from a site with publicly available images. My experience with app building and servers is more limited than with building models but my interest in it is just as strong, so I was excited at the challenge.

__Here's the finished [app](https://storage.googleapis.com/q-dev-challenge-hannah.appspot.com/index.html)__
__and here's my Github [repo](https://github.com/deenhe91/QloudAnimals)__

I needed to understand all the parts, moving and static of this machine, so I drew out some workflows. I knew I needed to get images, which meant using an image website API. Those images would have to be sent to Google and classified somehow. The results of that needed to be stored in a database (the company asked me to use [Firebase](https://firebase.google.com/), another new thing) and then that database would have to be accessed by a user looking up a specific animal in a simple front-end situation.

## flickr and the images

I chose flickr, specifically the `groups.pools.getPhotos` [API](flickr.com/services/api/explore/flickr.groups.pools.getPhotos) (even though I had to create a yahoo account in order to use it...) The quality of the images available is much higher than say, _imgur_, and the developer toolkit, or _'App Garden'_, is pretty well documented and easy to implement.


```python

import requests

flickr_key = '<YOUR ACCOUNT KEY>'
flickr_secret = '<YOUR ACCOUNT SECRET>'

flickr_file = []

for i in range(10):
	r = requests.get('https://api.flickr.com/services/rest/?method=flickr.groups.pools.getPhotos&api_key=<API KEY>
	&group_id=<GROUP ID>&per_page=500&page='+str(i)+'&format=json&nojsoncallback=1')

	flickr_json = r.json()
	flickr_file.append(flickr_json)
```

Using this API returned lots of information regarding each image within a specified 'group', which is basically a collection of images with a common theme that you can add to. I then parsed the information that was relevant to me and saved it, so I wouldn't have to overstay my flickr-api-welcome if it went wrong.


```python

img_info = flickr_list['photos']['photo']

with open('flickr.json', 'w') as outfile:
	json.dump(flickr_file, outfile)

```

I then had to compile URLs that pointed to each image so that they could be downloaded, but the format that flickr provided didn't work. However, looking at the page source...

![](https://github.com/deenhe91/deenhe91.github.io/blob/master/images/page_source.png?raw=true)

we can see the true format for the image URL. The format looks like this:

`https://farm{farm-id}.staticflickr.com/{server-id}/{id}_{secret}.jpg`

and all of the required fields were in the flickr API response. Hoorah!

The Google Vision API requires you to _either_ use images that are stored in Google Storage, _or_ send in the base64 encoded image as a string. So I downloaded the image at each URL using `shutil`, base64 encoded it using `b64` and then sent that string to Google.

```python

import shutil
import b64

b64_strings = [] # create list of b64_strings

for url in urls:

	response = requests.get(url, stream=True)
	
	with open('img.png', 'wb') as f:
		shutil.copyfileobj(response.raw, f)

	with open('img.png', 'rb') as f:
		encoded_string = base64.b64encode(f.read())

		b64_strings.append(encoded_string)

del response
```

Actually, I started off going through the process step by step, and saving along the way with the intention of turning it into a smooth running pipeline at the end. First I wanted to test whether each step worked without having to repeat any working processes. Because the name of the stored image, `img.png` remained the same, it was replaced at each iteration and I didn't hold onto anything in memory that I didn't need to.

## google and the vision

In order to use the API from my remote server, I had to create an environmental variable called 'GOOGLE_APPLICATION_CREDENTIALS' pointing to the file where my credentials were stored. You can see below, you just use `os` to do this. When you create your credentials with Google, a JSON file is automatically downloaded to your computer so just make sure that's in the right directory. 

```python

import os 
from googleapiclient import discovery 
from oauth2client.client import GoogleCredentials 

DISCOVERY_URL = 'https://vision.googleapis.com/$discovery/rest?version=v1'
# because I was using the google vision API

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '<json file where your credentials are stored>'

credentials = GoogleCredentials.get_application_default()	

service = discovery.build('vision', 'v1', credentials=credentials, discoveryServiceUrl=DISCOVERY_URL)

```

 
You can play around with the parameters a bit and specify what kind of detection you want (e.g. face, landmark, text). But I was just interested in animals so I set it to standard `'LABEL_DETECTION'`. I then set the parameters to max 5 labels per image, and saved the results in a python dictionary along with the score of that label and the url in a tuple, so that when it came to front end, I could return results by score and show the corresponding image.

`{'label': [(score, 'url'), (score, 'url'), (score, 'url')]}`

Every now and again a differently formatted or failed result would be returned so I introduced a conditional `if` statement where it would also tell me if an image had been skipped. That way I would know that there was something wrong with my code or the request if too many were being skipped.

```python

googlers = []

for string in b64_strings:

	service_request = service.images().annotate(body={'requests': [{'image': {'content': string.decode('UTF-8')},'features': [{'type': 'LABEL_DETECTION','maxResults': 5}]}]})

	response = service_request.execute()

	if 'labelAnnotations' in response['responses'][0]:
		labels = response['responses'][0]['labelAnnotations']
		print('string :'+ str(i))
	else:
		print('SKIPPED string : ' +str(i))
	googlers.append(labels)

```

Loads of the labels were not relevant to my own search criteria. As this needed to be an animal classifier, I was only really interested in animal images and animal labels. Originally I wanted to find a way to filter out irrelevant labels. Until I realised how much fun Google's misidentification could be.

### These beautiful butterflies enjoying a feeder?

![](https://github.com/deenhe91/deenhe91.github.io/blob/master/images/maitai.jpg?raw=true)

### Google reckons it's a Mai Tai.

![](https://github.com/deenhe91/deenhe91.github.io/blob/master/images/real_maitai.jpg?raw=true)

### Oh, and then this one is labeled as 'Air Force'..

![](https://github.com/deenhe91/deenhe91.github.io/blob/master/images/airforce.jpg?raw=true)

The funny thing about these 'mistakes' is that you can see why those images may have been classified the way they were. The butterflies do look a bit like a MaiTai and those birds being labeled Air Force is pretty hilarious. Maybe Google just has a great sense of humour. If you go and look at the [app](https://storage.googleapis.com/q-dev-challenge-hannah.appspot.com/index.html) you can see what kind of mistakes Google Vision still makes for yourself.

## firebase and the storages

Firebase is a backend system, absorbed by Google in 2014, for iOS, Android and the web that allows you to store and sync data instantly. It's a really nice service that you can work with through the firebase API or manually through their console. I had to use the console in the end because of python3 clashes - more on this in the __future and improvements__ section below.

Setting up a firebase project was easy, and I had a google project associated with the challenge and the Google Vision API already so I just linked the two togethere (there's a button for that) and navigated to the Database tab. As I had saved the dictionary as a JSON file, I could just import it into the firebase database manually and the result looked like this:

```python

import json

with open('firebase.json', 'w') as file:
	json.dump(dict_for_firebase, file)

```

![](https://github.com/deenhe91/deenhe91.github.io/blob/master/images/Screenshot_fb.png?raw=true)

Now for the front-end...

## users and the interface

One day to go and I had a whole front-end to build. 

I started building locally, and checking my progress using a python simple server. In python3, this is done by navigating to the directory where your app files are stored and then typing `$ python3 -m simple.server` on the command line.

In the HTML file you need to authorise your firebase access like so:

```html

    <script src="https://www.gstatic.com/firebasejs/3.3.0/firebase.js"></script>
      <script>

    // Initialize Firebase

    var config = {
      apiKey: "<API KEY>",
      authDomain: "<PROJECT NAME>.firebaseapp.com",
      databaseURL: "https://<PROJECT NAME>.firebaseio.com",
      storageBucket: "<PROJECT NAME>.appspot.com",
    };
    
    firebase.initializeApp(config);

  </script>
```

Then the connection to the firebase database is established in the javascript file. I did this using `firebase.database.ref()`, specifying where in your database I want to get to. I had `searchID` so that I could change the input depending on the label that the user would type into the search bar. 

```javascript

function getSearchImages(searchID){
	var animalRef = firebase.database().ref('data/'+searchID+'/');
	animalRef.on('value', function(snapshot) {
  
  		var response = snapshot.val();
  		for (var index in response) {
    		addImage(response[index][1]);
  		}
	});
};

```

Because some of the labels returned by Google Vision were pretty specific or obscure (ahum... _'hofmannophila pseudospretella'_, _'nova scotia duck tolling retriever'_, _'small to medium sized cats'_) I wanted to have an autocomplete function in the search bar. This was made possible with some magical jQuery with a [UI widget](https://jqueryui.com/autocomplete/). Upon pressing enter or choosing from the drop-down suggestions, the relevant images could be loaded.

```javascript


$(function() {
	var nameRef = firebase.database().ref('labels/')
	nameRef.on('value', function(snapshot) {
		var names = snapshot.val()

	    $("#tags").autocomplete({
	    	source: names,
	    	focus: function( event, ui ) {
		    	$( "#tags" ).val( ui.item.label );
		      	return false;
		    },
	    	select: function(event, ui){
	    		var searchID = ui.item.value
	    		clearImages();
	    		getSearchImages(searchID);
			}
		})
	})
});

```

### future and improvements

#### firebase API

What I really wanted to do was fill my firebase iteratively but with the time constraints and python clashes I changed tact. You can still see the code for using the firebase API with python in the repo. In order to get my front-end started more quickly I resorted to saving all my results in JSON and manually imported it into my firebase storage via the console. If I wanted an active pipeline, however, that updates the database on a regular basis, I would need to sort out the API.


#### image scaling

The image containers are set to a size most common to flickr images, but there are some cropped or portrait images that don't fit the format so those images appear stretched on the site. It would be good to find a different approach or be able to scale the image containers to the original image format to prevent this.

#### image ranking

Ideally I'd like to rank the returned images by the score returned by the Google API. I have them saved in the firebase database and I hope to implement this soon. 


