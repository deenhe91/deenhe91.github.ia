---

layout: post
title: let the machines decide
excerpt: ethical dilemmas of data collection
categories: [Data]
comments: true
image:
  feature: https://github.com/deenhe91/deenhe91.github.io/blob/master/images/2001.jpg?raw=true
  credit: 2001 Space Odyssey
  creditlink: 
---

### will they or won't they reoffend

What is the limit to what you can let the machines decide? In the States, the COMPAS recidivism algorithm is used to determine how likely a convicted criminal is to commit another crime once released. This score is _owned_ by [Northpointe, Inc.](http://www.northpointeinc.com/). This means that the justice system is heavily influenced by an unelected corporate power. Of course, because they are a private organisation who make their money, in part, from the use of their recidivism algorithm, the algorithm is black-boxed. So individual people are scored and tried based on a number between 1 and 10 that a machine supplies. [ProPublica's analysis](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm) of whether this tool was biased towards certain groups found that _"black defendants were far more likely than white defendants to be incorrectly judged to be at a higher risk of recidivism, while white defendants were more likely than black defendants to be incorrectly flagged as low risk"_. This is sadly unsurprising, given the race situation in the US, and it is one clear reason why we should exercise far more caution in how and when we rely on Machine Learning to show us the way.

### the right thing, for the wrong reasons...

#### ...or the wrong thing for the right ones?

Machine Learning is incredibly useful, and allows us to achieve beautiful and interesting things. Whether it is self-driving cars or [supplementing medical diagnoses](http://www.theatlantic.com/health/archive/2016/08/could-artificial-intelligence-improve-psychiatry/496964/?single_page=true), it holds much promise. I am also a firm believer that incorporating it into business architecture can do a lot of good. The giant evil Tesco has shown us that machine learning can be used to drastically increase efficiency when it comes to supermarket stocking, saving energy and massively reducing food waste. In my eyes, this is a BIG PLUS - even if it was probably done for less saintly reasons than saving the environment. The amount of waste in every element of society is shocking, and introducing a bit of smart to our tech will not hurt us. But when do the motives become important? Where do you draw the line on _who_ can collect _what_ data?

It certainly appears that the businesses I read about and work with show a keen interest in machine learning for two reasons.

![](https://github.com/deenhe91/deenhe91.github.io/blob/master/images/tap-tycoon-cheats.jpg?raw=true)

1. To reduce 'waste' in their spending and if that happens to coincide with reducing energy waste, food waste or any other kind of waste then that's just lucky. 

2. To gain power to make more money. Data is big. Everyone wants to collect and find ways to use it to make other people spend. more. money. 

And though I always considered myself as the kind of person who wants to do good things for people and the world, this is where I find myself now - the dragon's den. Here I need to consider how I can collect data so that I can assure that companies can make the most money. 

### being the bad guy?

Let me give you an example of a little quandary I am struggling with. Shops collect data on transactions. This is not new. People have been keeping accurate records of transactions for their business for hundreds, or thousands, of years. Even the acient Sumerians of Mesopotamia kept records of taxes and trade. Today, companies are thinking of ever-cleverer ways of using this stored information. Netflix recommends shows based on what we watch and Spotify generates our Discover Weekly lists based on our listening history, companies want to give us the most relevant advertising possible. And they can do that by, for example, looking at what their customers buy and how often. It's a pretty simple and oft-used framework and I think I have made my peace with it. If I have to look at advertising, I would much rather be shown advertising that is relevant to me and what I like. However, say you add the identity of the cashier to those transaction details. Now as a company, you know which cashiers do better than others in which stores. Which cashiers attract more customers. You introduce this little quirk and jobs are on the line. Incorporate that info into a classifier and it can tell you whether it's worth firing someone or not. Booya. 

The thing is, though this may seem genius to some companies. The interactions of employees and who is valuable to your store and who isn't is far more nuanced than this metric may have you believe. Say you use the frequency of transactions processed by a cashier as a measure of that cashier's 'popularity' with the customers, or work ethic. And maybe you added a metric for availability, how many hours they work. You probably add some time aspect because how many transactions get processed would probably rely heavily on the time of day... Of course you add their salary. These are all parameters worthy of our firing classifier but the algorithm is still not going to capture who keeps spirits high in the team. Maybe you have an employee who is always helping the others. A problem-solver. This person may be invaluable to the team but to the classifier they'll be a good one for firing because the number of transactions they process is way below the others. And of course, none of this takes into account the personal situation of the employees. Two people are classified as fireable, one more so than the other, but that one happens to be going through a divorce and actually needs the money and support of a daily routine much more so than the other person. 

There are many conversations to be had regarding our responsibility around data science, and until we have them, perhaps we should keep the final decision making to ourselves. 




